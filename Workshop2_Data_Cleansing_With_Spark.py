# -*- coding: utf-8 -*-
# ‡∏°‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏° Workshop 2 ‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡∏¢

![](https://file.designil.com/VrNxQe+)

# Step 1) ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Spark ‡πÅ‡∏•‡∏∞ PySpark

Google Colab ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á Python ‡πÅ‡∏•‡∏∞ Bash ‡∏ö‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ó‡∏µ‡πà Google ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤

‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Virtual Machine (VM)
"""

!apt-get update                                                                          # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó Package ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô VM ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
!apt-get install openjdk-8-jdk-headless -qq > /dev/null                                  # ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Java Development Kit (‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Spark)
!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz # ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Spark 3.1.2
!tar xzvf spark-3.1.2-bin-hadoop2.7.tgz                                                  # Unzip ‡πÑ‡∏ü‡∏•‡πå Spark 3.1.2
!pip install -q findspark==1.3.0                                                         # ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Package Python ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Spark

# Set enviroment variable ‡πÉ‡∏´‡πâ Python ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å Spark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop2.7"

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PySpark ‡∏•‡∏á‡πÉ‡∏ô Python
!pip install pyspark==3.1.2

"""## ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Spark"""

# Server ‡∏Ç‡∏≠‡∏á Google Colab ‡∏°‡∏µ‡∏Å‡∏µ‡πà Core
!cat /proc/cpuinfo

"""‡πÉ‡∏ä‡πâ `local[*]` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö multi-core (Spark ‡∏à‡∏∞‡πÉ‡∏ä‡πâ CPU ‡∏ó‡∏∏‡∏Å core ‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á)

`.getOrCreate()` ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ core ‡∏Å‡πá‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏≤‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤

"""

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Spark Session ‡πÄ‡∏û‡∏¥‡πâ‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Spark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

# ‡∏î‡∏π‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ô Python
# Pyhton version 3.9.16 180423
import sys
sys.version_info

# ‡∏î‡∏π‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ô Spark
spark.version

"""## Load Workshop 2 Data

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á:

wget = ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå

wget -O = ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå

unzip ‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô zip
"""

# Download Data File
!wget -O data.zip https://file.designil.com/zdOfUE+
!unzip data.zip

"""### Load data ‡πÉ‡∏™‡πà Spark

‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á `spark.read.csv` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV

Arguments:

Header = True << ‡∏ö‡∏≠‡∏Å‡πÉ‡∏´‡πâ Spark ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏õ‡πá‡∏ô Header

Inferschema = True << ‡∏ö‡∏≠‡∏Å‡πÉ‡∏´‡πâ Spark ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡∏ß‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞ column ‡∏°‡∏µ type ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏∞‡πÑ‡∏£ [ ‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô False, ‡∏ó‡∏∏‡∏Å column ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô string (‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠) ]

Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html


'/content/ws2_data.csv' ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ copy path ‡πÉ‡∏ô file

header = True : ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô Header

inferSchema = True: ‡πÉ‡∏´‡πâ Spark ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô data type ‡∏≠‡∏±‡∏¢‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
"""

dt = spark.read.csv('/content/ws2_data.csv', header = True, inferSchema = True, )

"""---

![](https://file.designil.com/2c6qGS+)

# Step 2) Data Profiling

Data Profiling ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÑ‡∏´‡∏ô‡∏ö‡πâ‡∏≤‡∏á ‡∏Ñ‡πà‡∏≤‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á ‡∏Ø‡∏•‡∏Ø ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡πÑ‡∏´‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: max, min, average, sum, ‡∏°‡∏µ missing value ‡∏°‡∏±‡πâ‡∏¢ ‡∏Ø‡∏•‡∏Ø
"""

# ‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á
dt

"""> Columns:
1. timestamp
2. user_id
3. book_id
4. country
5. price
"""

# ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
dt.show()

# ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 100 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
dt.show(100)

# ‡∏î‡∏π‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
dt.dtypes

# ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (Schema)
dt.printSchema()

"""nullable ‡∏Ñ‡∏∑‡∏≠ ‡∏Ñ‡πà‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡πá‡∏ô null ‡πÑ‡∏î‡πâ




"""

# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡πÅ‡∏•‡∏∞ column
print((dt.count(), len(dt.columns)))

# ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
# NaN = Not a Number
dt.describe().show()

# ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
# ReferenceL: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.summary.html

dt.summary().show()

# ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏â‡∏û‡∏≤‡∏∞ column ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
dt.select("price").describe().show()

"""### Exercise 1

‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÑ‡∏´‡∏ô‡∏°‡∏µ Missing Value ‡∏ö‡πâ‡∏≤‡∏á? ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ Missing Value ‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏´‡∏ô‡πà‡∏≠‡∏¢


"""

# Answer here
dt.summary("count").show()
dt.where(dt.user_id.isNull()).show()

"""---

![](https://file.designil.com/D9H1d3+)

# Step 3) EDA - Exploratory Data Analysis

## Non-Graphical EDA

‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á Spark ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏î‡πâ
"""

# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
dt.where(dt.price >= 1).show()

# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠
dt.where(dt.country == 'Canada').show()

"""### Exercise 2: 
1. ‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô ‡∏°‡∏µ‡∏Å‡∏µ‡πà‡πÅ‡∏ñ‡∏ß
2. ‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏° ‡∏°‡∏µ‡∏Å‡∏µ‡πà‡πÅ‡∏ñ‡∏ß
"""

# Answer here
# 1.
dt.where(dt.timestamp.startswith("2021-04")).show()
#2.
dt.where(dt.timestamp.startswith("2021-08")).show()

"""`startswith()` ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏ô Spark ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÑ‡∏õ‡∏î‡∏π‡πÉ‡∏ô API Reference ‡πÑ‡∏î‡πâ 

`dt.where(dt.timestamp.startswith("2021-04")).show()` ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ 

`dt.where(dt.timestamp.startswith("2021-04")).count()` ‡πÉ‡∏ä‡πâ‡πÄ‡∏ä‡πá‡∏Ñ‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ‡πÑ‡∏î‡πâ

## Graphical EDA


Spark ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏á‡∏≤‡∏ô plot ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ package `seaborn` `matplotlib` ‡πÅ‡∏•‡∏∞ `pandas` ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ plot ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ó‡∏ô
"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# ‡πÅ‡∏õ‡∏•‡∏á Spark Dataframe ‡πÄ‡∏õ‡πá‡∏ô Pandas Dataframe - ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 6 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
dt_pd = dt.toPandas()

# ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
dt_pd.head()

# Boxplot - ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
sns.boxplot(x = dt_pd['book_id'])

# Histogram - ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
# bins = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô bar ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á
sns.histplot(dt_pd['price'], bins=10)

"""### Exercise 3: 
book_id ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡∏≤‡∏°‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤?

‡∏•‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Plot ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á book_id ‡∏Å‡∏±‡∏ö price
"""

# Answer here
sns.scatterplot(x = dt_pd.book_id, y = dt_pd.price)
# book_id ‡∏Å‡∏±‡∏ö price ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ô

# Other ans
sns.scatterplot(data=dt_pd, x = 'book_id', y = 'price')

"""#### Bonus: ‡∏™‡∏£‡πâ‡∏≤‡∏á interactive chart"""

# Plotly - interactive chart
import plotly.express as px
fig = px.scatter(dt_pd, 'book_id', 'price')
fig.show()

"""---

![](https://file.designil.com/Huzkx0+)

# Step 4) Data Cleansing with Spark

‡∏°‡∏≤‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ Spark ‡∏Å‡∏±‡∏ô‡πÄ‡∏ñ‡∏≠‡∏∞

### ‡πÅ‡∏õ‡∏•‡∏á Data Type

‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÅ‡∏ö‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ñ‡∏∑‡∏≠ **Data Type ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£**
"""

dt

# Show top 5 rows
dt.show(5)

# Show Schema
dt.printSchema()

"""‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ `Timestamp` ‡∏ñ‡∏π‡∏Å‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ (String) ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ (date time) ‡∏à‡∏∞‡∏ó‡∏≥‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏î‡∏µ?

‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏∑‡πà‡∏ô ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏î‡∏π‡∏Å‡πà‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Timestamp ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏•‡∏Ç‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡πà‡∏≠‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏•‡∏Ç‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏Å‡πà‡∏≠‡∏ô (DD/MM/YYYY ‡∏´‡∏£‡∏∑‡∏≠ MM/DD/YYYY)
"""

dt.select("timestamp").show(10)

"""‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡πà‡∏ô to_timestamp ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô pyspark.sql.functions ‡∏Å‡∏±‡∏ô

Reference: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.to_timestamp.html
"""

# ‡πÅ‡∏õ‡∏•‡∏á string ‡πÄ‡∏õ‡πá‡∏ô datetime
# create new column `withColumn`
from pyspark.sql import functions as f

dt_clean = dt.withColumn("timestamp",
                        f.to_timestamp(dt.timestamp, 'yyyy-MM-dd HH:mm:ss')
                        )
dt_clean.show()

dt_clean.printSchema()

"""## BONUS: ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Datetime"""

# ‡∏ô‡∏±‡∏ö‡∏¢‡∏≠‡∏î transaction ‡∏ä‡πà‡∏ß‡∏á‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÅ‡∏£‡∏Å ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô
dt_clean.where( (f.dayofmonth(dt_clean.timestamp) <= 15) & ( f.month(dt_clean.timestamp) == 6 ) ).count()

"""## Anomalies Check

‡πÉ‡∏ä‡πâ Spark ‡∏ï‡∏≤‡∏°‡∏´‡∏≤‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

### ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ 1) Syntactical Anomalies
**Lexical errors** ‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î

#### Exercise 4

‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏Å‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å
"""

# ‡πÉ‡∏ô Data set ‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Å‡∏µ‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®
dt_clean.select("Country").distinct().count()

# ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà ... ‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
# sort = ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
# show() ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÅ‡∏Ñ‡πà 20 ‡∏≠‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡πÉ‡∏™‡πà False ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏° ‡πÜ (‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠)
dt_clean.select("Country").distinct().sort("Country").show(58, False )

"""‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏¥‡∏î ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£

Japane
"""

# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ... ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡∏ú‡∏¥‡∏î
dt_clean.where(dt_clean['Country'] == 'Japane').show()

"""‡πÑ‡∏î‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏´‡πâ‡∏™‡∏∞‡∏Å‡∏î‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"""

# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ... ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡∏ú‡∏¥‡∏î ‡πÅ‡∏•‡∏∞ ...2 ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
from pyspark.sql.functions import when

dt_clean_country = dt_clean.withColumn("CountryUpdate", 
                                       when(dt_clean['Country'] == 'Japane', 'Japan').otherwise(dt_clean['Country'])
                                       )

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
dt_clean_country.select("CountryUpdate").distinct().sort("CountryUpdate").show(58, False)

# ‡∏î‡∏π‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ
dt_clean_country.show()

# ‡πÄ‡∏≠‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå CountryUpdate ‡πÑ‡∏õ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Country
dt_clean = dt_clean_country.drop("Country").withColumnRenamed('CountryUpdate', 'Country')

# ‡∏î‡∏π‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
dt_clean.show()

"""#### ‡∏à‡∏ö Exercise 4

### ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ 2) Semantic Anomalies

**Integrity constraints**: ‡∏Ñ‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô
- user_id: ‡∏Ñ‡πà‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ 8 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
"""

# ‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• user_id ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
dt_clean.select("user_id").show(10)

# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô user_id ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
dt_clean.select("user_id").count()

"""#### Exercise 5

‡∏´‡∏≤‡∏ß‡πà‡∏≤ user_id ‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏°‡∏±‡πâ‡∏¢ ‡πÅ‡∏•‡∏∞‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á

‡∏î‡∏π‡∏ß‡πà‡∏≤ user_id ‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡∏°‡∏µ‡∏Å‡∏µ‡πà‡πÅ‡∏ñ‡∏ß

‡∏Ñ‡∏≥‡πÉ‡∏ö‡πâ: ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå https://www.regex101.com ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Regular Expression ‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
"""

# ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà ... ‡∏î‡πâ‡∏ß‡∏¢ Regular Expression ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö user_id ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
# ‡∏Ñ‡∏≥‡πÉ‡∏ö‡πâ: ‡πÉ‡∏ô Regular Expression ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡∏°‡∏µ ^ ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ ‡πÅ‡∏•‡∏∞‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ $
dt_clean.where(dt_clean["user_id"].rlike("^[a-zA-Z0-9]{8}$")).count()

"""‡∏°‡∏≤‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡πâ‡∏≤‡∏á ‡∏ß‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô

![](https://file.designil.com/MmVhZf+)
"""

# ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: Cell ‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 15 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ

# ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà ... ‡∏î‡πâ‡∏ß‡∏¢ Regular Expression ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö user_id ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
dt_correct_userid = dt_clean.filter(dt_clean["user_id"].rlike("^[a-z0-9]{8}$"))
dt_incorrect_userid = dt_clean.subtract(dt_correct_userid)

dt_incorrect_userid.show(10)

"""‡∏°‡∏≤‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç user_id ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏ñ‡∏≠‡∏∞ (‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô null ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç)"""

# ‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏≤‡∏Å Exercise 4)
dt_clean_userid = dt_clean.withColumn("User_id_update", 
                                       when(dt_clean['user_id'] == 'ca86d17200', 'ca86d172').otherwise(dt_clean['user_id'])
                                       )

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
dt_correct_userid = dt_clean_userid.filter(dt_clean_userid["user_id"].rlike("^[a-z0-9]{8}$"))
dt_incorrect_userid = dt_clean_userid.subtract(dt_correct_userid)

dt_incorrect_userid.show(10)

# ‡πÄ‡∏≠‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå user_id_update ‡πÑ‡∏õ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà user_id (‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏≤‡∏Å Exercise 4)
dt_clean = dt_clean_userid.drop("user_id").withColumnRenamed('User_id_update', 'user_id')

dt_clean.show(10)

"""#### ‡∏à‡∏ö Exercise 5

### ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ 3) Missing values

‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Missing Values (‡∏´‡∏≤‡∏Å‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)

‡∏Ñ‡πà‡∏≤ Missing Value ‡∏Ñ‡∏∑‡∏≠ ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏Å‡∏µ‡πà‡∏Ñ‡πà‡∏≤
"""

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1 ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ Missing Value
# ‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ List Comparehension - ‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô Pre-course Python https://school.datath.com/courses/road-to-data-engineer-2/contents/6129b780564a8
# ‡πÄ‡∏ä‡πà‡∏ô [ print(i) for i in [1,2,3] ]

# col = ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á Spark ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
# sum = ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á Spark ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡∏ú‡∏•‡∏£‡∏ß‡∏°
from pyspark.sql.functions import col, sum

dt_nulllist = dt_clean.select([ sum(col(colname).isNull().cast("int")).alias(colname) for colname in dt_clean.columns ])
dt_nulllist.show()

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2 ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ Missing Value - ‡∏à‡∏≤‡∏Å Exercise 1 ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Å‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏ö‡∏ß‡∏Å‡∏•‡∏ö‡πÄ‡∏≠‡∏á
dt_clean.summary("count").show()

# ‡∏î‡∏π‡∏ä‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡πà‡∏≤‡πÅ‡∏ñ‡∏ß‡πÑ‡∏´‡∏ô‡∏°‡∏µ user_id ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ (‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Exercise 1)

dt_clean.where( dt_clean.user_id.isNull() ).show()

"""#### Exercise 6:
‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡∏° Data Analyst ‡πÅ‡∏à‡πâ‡∏á‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÅ‡∏ó‡∏ô user_id ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô NULL ‡∏î‡πâ‡∏ß‡∏¢ 00000000 ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
"""

# Answer here
dt_clean_user_id = dt_clean.withColumn("UserUpdate", 
                                       when(dt_clean['user_id'].isNull(), '00000000').otherwise(dt_clean['user_id'])
                                       )

dt_clean = dt_clean_user_id.drop("user_id").withColumnRenamed("UserUpdate", "user_id")

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ user ID ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô NULL ‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏±‡πâ‡∏¢
dt_clean.where( dt_clean.user_id.isNull() ).show()

"""### ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ 4) Outliers:

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡πà‡∏≥‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà

‡∏°‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Boxplot ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Outlier ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠
"""

# Cell ‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏£‡∏±‡∏ô‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏ô‡∏≤‡∏ô ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡πÄ‡∏¢‡∏≠‡∏∞
dt_clean_pd = dt_clean.toPandas()

sns.boxplot(x = dt_clean_pd['price'])

"""‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏ö‡∏≤‡∏á‡πÄ‡∏•‡πà‡∏°‡∏ó‡∏µ‡πà‡∏£‡∏≤‡∏Ñ‡∏≤‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥‡πÑ‡∏õ‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å ‡∏•‡∏≠‡∏á‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ book_id ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á ‡∏ó‡∏µ‡πà‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô $80"""

dt_clean.where( dt_clean.price > 80 ).select("book_id").distinct().show()

"""‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥ Book_ID ‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏õ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô $80 ‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏°‡∏±‡πâ‡∏¢

‡∏ñ‡πâ‡∏≤‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Workshop 1 ‡∏Å‡πá‡∏à‡∏∞‡∏û‡∏ö‡∏ß‡πà‡∏≤ Book_ID = 635 ‡∏Ñ‡∏∑‡∏≠ ‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠ "The Power Broker"
https://www.audible.com/pd/The-Power-Broker-Audiobook/B0051JH67K?ipRedirectOverride=true&overrideBaseCountry=true&pf_rd_p=2756bc30-e1e4-4174-bb22-bce00b971761&pf_rd_r=MF7KC1JQF3A6GK2ET8XM

![](https://file.designil.com/7h1WIp+)

The Power Broker ‡∏°‡∏µ‡∏£‡∏≤‡∏Ñ‡∏≤ $84 ‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á 66 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á

**‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Outlier ‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î ‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡∏≠‡∏∞‡πÑ‡∏£**

### ‡∏°‡∏≤‡∏•‡∏≠‡∏á Clean ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ Spark SQL

![alt text](https://cdn-std.droplr.net/files/acc_513973/881iHw)
"""

# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Spark DataFrame ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô TempView ‡∏Å‡πà‡∏≠‡∏ô
dt.createOrReplaceTempView("data")
dt_sql = spark.sql("SELECT * FROM data")
dt_sql.show()

# ‡∏•‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® Exercise 4 ‡πÄ‡∏õ‡πá‡∏ô SQL
dt_sql_country = spark.sql("""
SELECT distinct country
FROM data
ORDER BY country
""")
dt_sql_country.show(100)

# ‡∏•‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡∏à‡∏≤‡∏Å Exercise 4 ‡πÄ‡∏õ‡πá‡∏ô SQL
dt_sql_result = spark.sql("""
SELECT timestamp, user_id, book_id,
  CASE WHEN country = 'Japane' THEN 'Japan' ELSE country END AS country,
price
FROM data
""")
dt_sql_result.show()

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ß‡πà‡∏≤‡∏ñ‡∏π‡∏Å‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏±‡πâ‡∏¢
dt_sql_result.select("country").distinct().sort("country").show(58, False)

"""#### Exercise 7

‡∏ó‡∏≥ Exercise 5 ‡∏î‡πâ‡∏ß‡∏¢ SQL

‡∏Ñ‡∏≥‡πÉ‡∏ö‡πâ: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á RLIKE ‡πÉ‡∏ô SQL ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Regular Expression ‡πÑ‡∏î‡πâ
"""

# Answer here: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• user_id ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 8 ‡∏´‡∏•‡∏±‡∏Å‡∏°‡∏±‡πâ‡∏¢
dt_sql_result = spark.sql("""
SELECT *
FROM data
WHERE user_id NOT RLIKE '^[a-z0-9]{8}$'
""")
dt_sql_result.show()

# Answer here: ‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏≤ (‡∏Ñ‡∏≥‡πÉ‡∏ö‡πâ: ‡πÉ‡∏ä‡πâ CASE WHEN)
dt_sql_uid_result = spark.sql("""
SELECT timestamp,
  CASE WHEN user_id NOT RLIKE '^[a-z0-9]{8}$' THEN '00000000' ELSE user_id END AS  user_id,
book_id, country, price
FROM data
""")
dt_sql_uid_result.show()

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î ‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
dt_sql_uid_result.where( dt_sql_uid_result.user_id == 'ca86d17200' ).show()

"""---

![](https://file.designil.com/TmpQfK+)

# Step 5) Save data ‡πÄ‡∏õ‡πá‡∏ô CSV

‡πÇ‡∏î‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥‡πÅ‡∏•‡πâ‡∏ß Spark ‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Save ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
"""

# ‡πÄ‡∏ã‡∏ü‡πÄ‡∏õ‡πá‡∏ô partitioned files (‡πÉ‡∏ä‡πâ multiple workers)
dt_clean.write.csv('Cleaned_data.csv', header = True)

"""‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ Spark ‡πÄ‡∏ã‡∏ü‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÑ‡∏î‡πâ"""

# ‡πÄ‡∏ã‡∏ü‡πÄ‡∏õ‡πá‡∏ô 1 ‡πÑ‡∏ü‡∏•‡πå (‡πÉ‡∏ä‡πâ single worker)
dt_clean.coalesce(1).write.csv('Cleaned_Data_Single.csv', header = True)

"""‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏à‡∏ö‡πÅ‡∏Ñ‡πà‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡πÅ‡∏ñ‡∏°‡∏≠‡∏µ‡∏Å‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á...

### Bonus: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ Part
‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏°‡∏µ
- /content/Cleaned_Data.csv/part-00000-....csv
- /content/Cleaned_Data.csv/part-00001-....csv
"""

all_parts = spark.read.csv('/content/Cleaned_data.csv/part-*.csv', header = True, inferSchema = True)

all_parts.count()

print('‡∏à‡∏ö Workshop 2 ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡πâ‡∏≤‡∏ö üòç')